{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104024,"databundleVersionId":12520411,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Setting up essential libraries for data processing, modeling, and visualization. `lightgbm` is used for fast, high-performance gradient boosting.","metadata":{}},{"cell_type":"code","source":"# === ENVIRONMENT SETUP ===\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\n!pip install -q lightgbm\nfrom lightgbm import LGBMRegressor, early_stopping, log_evaluation\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport random\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\nfrom scipy.signal import savgol_filter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load training and test datasets along with submission column names.","metadata":{}},{"cell_type":"code","source":"# === PATHS ===\ndata_path = '/kaggle/input/volatility-smile-prediction'\noutput_path = '/kaggle/working'\n\n# === LOAD DATA ===\ntrain_df = pd.read_parquet(f'{data_path}/train_data.parquet')\ntest_df = pd.read_parquet(f'{data_path}/test_data.parquet')\nsample_submission = pd.read_csv(f'{data_path}/sample_submission.csv')\nsubmission_cols = sample_submission.columns.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing\n\n### This section summarizes the main data preprocessing steps applied before modeling:\n\n## 1. Outlier Removal\n### - Remove rows where any `call_iv_` or `put_iv_` column is outside the [0.0, 1.0] range.\n\n## 2. Cross-IV Feature Engineering\n### - For each strike, add new features: each call IV gets the corresponding put IV, and vice versa.\n\n## 3. Feature Selection\n### - Keep columns starting with `X` and valid target columns.\n### - Add the new cross-IV features to the feature set.\n\n## 4. Drop Sparse and Low-Variance Features\n### - Remove `X` columns with more than 50% zeros.\n### - Drop features with standard deviation less than 0.01.\n\n## 5. Denoising\n### - Apply Exponential Weighted Moving Average (EWM) smoothing to all IV columns.\n\n## 6. Remove Highly Correlated Features\n### - Drop one of any pair of features with correlation greater than 0.98.\n\n## 7. Clipping\n### - Clip all feature values to the 1st and 99th percentiles to reduce the impact of outliers.\n\n## 8. Save Cleaned Data\n### - Export the cleaned train and test datasets for further modeling.\n\n","metadata":{}},{"cell_type":"code","source":"# === REMOVE OUTLIERS FUNCTION ===\ndef remove_iv_outliers(df):\n    iv_cols = [col for col in df.columns if col.startswith('call_iv_') or col.startswith('put_iv_')]\n    outlier_mask = np.zeros(len(df), dtype=bool)\n    for col in iv_cols:\n        out_of_bounds = (df[col] < 0.0) | (df[col] > 1.0)\n        if out_of_bounds.any():\n            print(f\"{col}: {out_of_bounds.sum()} outlier rows ‚Äî removing\")\n            outlier_mask |= out_of_bounds\n    df_cleaned = df[~outlier_mask].reset_index(drop=True)\n    return df_cleaned\n\n# === CROSS-IV FEATURE ADDITION FUNCTION ===\ndef add_cross_iv_features(df):\n    call_cols = [col for col in df.columns if col.startswith('call_iv_')]\n    put_cols = [col for col in df.columns if col.startswith('put_iv_')]\n\n    call_strikes = {col.split('_')[-1]: col for col in call_cols}\n    put_strikes = {col.split('_')[-1]: col for col in put_cols}\n\n    common_strikes = set(call_strikes.keys()) & set(put_strikes.keys())\n    new_features = []\n\n    for strike in common_strikes:\n        call_col = call_strikes[strike]\n        put_col = put_strikes[strike]\n\n        call_extra = f\"{call_col}_extra\"\n        put_extra = f\"{put_col}_extra\"\n\n        df[call_extra] = df[put_col]\n        df[put_extra] = df[call_col]\n\n        new_features.extend([call_extra, put_extra])\n\n    return df, new_features\n\n# === APPLY CLEANING ===\ntrain_df = remove_iv_outliers(train_df)\n# test_df = remove_iv_outliers(test_df)  # Optional\n\n# === APPLY CROSS-FEATURES ===\ntrain_df, cross_features_train = add_cross_iv_features(train_df)\ntest_df, cross_features_test = add_cross_iv_features(test_df)\ncommon_cross_features = list(set(cross_features_train) & set(cross_features_test))\n\n\n# === FEATURE AND TARGET COLUMNS ===\nfeature_cols = [col for col in train_df.columns if col.startswith('X')]\ntarget_cols = [col for col in submission_cols if col in train_df.columns]\nfeature_cols += common_cross_features\n\n# === REMOVE 'X' COLUMNS WITH >50% ZEROS ===\nx_cols = [col for col in feature_cols if col.startswith('X')]\ncols_to_drop = []\n\nfor col in x_cols:\n    zero_fraction = (train_df[col] == 0).mean()\n    if zero_fraction > 0.5:\n        print(f\"Dropping column {col} ‚Äî {zero_fraction:.2%} values are zero\")\n        cols_to_drop.append(col)\n        \n# === REMOVE FEATURES WITH STD < 0.01 ===\nlow_std_cols = []\nfor col in feature_cols:\n    if train_df[col].std() < 0.01:\n        low_std_cols.append(col)\n        print(f\"Dropping column {col} ‚Äî std is {train_df[col].std():.5f} < 0.01\")\n\n\ntrain_df.drop(columns=cols_to_drop, inplace=True)\ntest_df.drop(columns=cols_to_drop, inplace=True)\n\n# ====FAST DENOISING==========\ndef fast_denoise_iv(df, span=3, verbose=True):\n    iv_cols = [col for col in df.columns if col.startswith('call_iv_') or col.startswith('put_iv_')]\n\n    if verbose:\n        print(f\"üìâ Applying EWM denoising to {len(iv_cols)} columns...\")\n\n    df.sort_values(\"timestamp\", inplace=True)\n    df[iv_cols] = df[iv_cols].ewm(span=span, adjust=False).mean()\n    return df\n\n\n\n# === APPLY DENOISING ===\ntrain_df = fast_denoise_iv(train_df, span=3)\ntest_df = fast_denoise_iv(test_df, span=3)\n\n\n\nfeature_cols = [col for col in feature_cols if col not in cols_to_drop]\n\n# === REMOVE HIGHLY CORRELATED FEATURES (corr > 0.98) ===\n\nprint(\"üîÅ Removing highly correlated features...\")\ncorr_matrix = train_df[feature_cols].corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop_corr = [column for column in upper.columns if any(upper[column] > 0.98)]\n\nfor col in to_drop_corr:\n    print(f\"Dropping {col} ‚Äî high correlation with another feature\")\n\n\nfor col in feature_cols:\n    q1 = train_df[col].quantile(0.01)\n    q99 = train_df[col].quantile(0.99)\n    train_df[col] = np.clip(train_df[col], q1, q99)\n    test_df[col] = np.clip(test_df[col], q1, q99)\n\nprint(feature_cols)\n# === SAVE CLEANED INPUTS ===\ntrain_df.to_csv(f'{output_path}/train_inputs_cleaned.csv', index=False)\ntest_df.to_csv(f'{output_path}/test_inputs_cleaned.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering & Model Training\n\n## 1. IV Aggregates\n### - Calculate the mean and standard deviation across all target IV columns for each row (`iv_mean`, `iv_std`).\n\n## 2. Missing Value Indicators\n### - For every feature, add a new column indicating if the value is missing (`_na` suffix).\n\n## 3. Strike Extraction\n### - Extract the strike value from each target column name and add it as a feature.\n\n## 4. Model Training Loop\n### - For each target:\n  #### - Filter out rows with missing target values.\n  #### - Use KFold cross-validation to split the data (first fold only).\n  #### - Train a LightGBM regressor with specified hyperparameters.\n  #### - Predict on the test set and store results.\n","metadata":{}},{"cell_type":"code","source":"# === FEATURE ENGINEERING ===\n\ntrain_df[\"iv_mean\"] = train_df[target_cols].mean(axis=1)\ntrain_df[\"iv_std\"] = train_df[target_cols].std(axis=1)\ntest_df[\"iv_mean\"] = test_df[target_cols].mean(axis=1)\ntest_df[\"iv_std\"] = test_df[target_cols].std(axis=1)\nfeature_cols += [\"iv_mean\", \"iv_std\"]\n\nfor col in feature_cols.copy():\n    train_df[col + \"_na\"] = train_df[col].isna().astype(int)\n    test_df[col + \"_na\"] = test_df[col].isna().astype(int)\n    feature_cols.append(col + \"_na\")\n\n# === STRIKE EXTRACTION FUNCTION ===\ndef extract_strike(col_name):\n    try:\n        return int(col_name.split('_')[-1])\n    except:\n        return np.nan\n\n# === MODEL TRAINING LOOP ===\npreds_df = pd.DataFrame(index=test_df.index)\n\nfor target in target_cols:\n    print(f\"\\nüîß Training model for target: {target}\")\n    strike_val = extract_strike(target)\n    train_df[\"strike\"] = strike_val\n    test_df[\"strike\"] = strike_val\n    if \"strike\" not in feature_cols:\n        feature_cols.append(\"strike\")\n\n    train_target_df = train_df.dropna(subset=[target])\n    if train_target_df.empty:\n        print(f\"‚ö†Ô∏è No data to train for {target}\")\n        preds_df[target] = 0.2 + (random.random() - 0.5) * 0.1\n        continue\n\n    X = train_target_df[feature_cols]\n    y = train_target_df[target]\n    \n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    for train_idx, val_idx in kf.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        break\n\n    model = LGBMRegressor(\n    objective='regression',\n    learning_rate=0.01,\n    num_leaves=128,\n    max_depth=10,\n    feature_fraction=0.9,\n    bagging_fraction=0.9,\n    bagging_freq=1,\n    n_estimators=4000,\n    lambda_l1=1.0,\n    lambda_l2=1.0,\n    min_child_samples=20,\n    random_state=42,\n    n_jobs=-1,\n    )\n\n\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric='rmse',  # MSE in LightGBM is 'l2'\n        callbacks=[early_stopping(stopping_rounds=200), log_evaluation(100)]\n    )\n\n    preds_df[target] = model.predict(test_df[feature_cols])\n    del model, train_target_df, X, y, X_train, X_val, y_train, y_val\n    gc.collect()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission in the Competition","metadata":{}},{"cell_type":"code","source":"# === SUBMISSION ===\nfor col in submission_cols:\n    if col not in preds_df.columns:\n        preds_df[col] = 0.2 + (random.random() - 0.5) * 0.1\n\n\npreds_df = preds_df[submission_cols]\npreds_df['timestamp'] = range(len(preds_df))\n\nsubmission_file = os.path.join(output_path, 'submission.csv')\npreds_df.to_csv(submission_file, index=False)\nprint(\"\\n‚úÖ Submission saved to:\", submission_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}